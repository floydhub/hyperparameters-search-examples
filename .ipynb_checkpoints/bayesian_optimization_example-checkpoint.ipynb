{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization Example\n",
    "\n",
    "In this example we will perform a Bayesian Optimization using [Hyperas](https://github.com/maxpumperla/hyperas) on the [breast cancer](https://github.com/autonomio/datasets/blob/master/autonomio-datasets/breast_cancer.csv) classification task. You can run this example on CPU. It will take more or less 3 minutes.\n",
    "\n",
    "We will continue to use the same example of the [Grid Search](./grid_search_example.ipynb) and [Random Search](./random_search_example.ipynb) notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "Import the packages we need for the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import nan\n",
    "import wrangle as wr\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Load, clean and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "    This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    # Base Url for the dataset\n",
    "    BASE = 'https://raw.githubusercontent.com/autonomio/datasets/master/autonomio-datasets/'\n",
    "    \n",
    "    def breast_cancer():\n",
    "        \"\"\"Download and preprocess(cleaning) the dataset\"\"\"\n",
    "        df = pd.read_csv(BASE + 'breast_cancer.csv')\n",
    "\n",
    "        # then some minimal data cleanup\n",
    "        df.drop(\"Unnamed: 32\", axis=1, inplace=True)\n",
    "        df.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "        # separate to x and y\n",
    "        y = df.diagnosis.values\n",
    "        x = df.drop('diagnosis', axis=1).values\n",
    "\n",
    "        # convert the string labels to binary\n",
    "        y = (y == 'M').astype(int)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    # Load the dataset\n",
    "    x, y = breast_cancer()\n",
    "\n",
    "    # Normalize every feature to mean 0, std 1\n",
    "    x = wr.mean_zero(pd.DataFrame(x)).values\n",
    "\n",
    "    input_dim = x.shape[1] # number of columns\n",
    "\n",
    "    # Train - Test split: 66 - 33\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=7)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "Define the model and the variables to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # L1\n",
    "    model.add(Dense({{choice([8,9,10])}}, \n",
    "                    input_dim=input_dim, \n",
    "                    kernel_initializer={{choice(['uniform', 'normal'])}}, \n",
    "                    activation={{choice(['relu', 'elu'])}}))\n",
    "    # Dropout\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    # L2\n",
    "    model.add(Dense(1, \n",
    "                    kernel_initializer={{choice(['uniform', 'normal'])}}, \n",
    "                    activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer={{choice(['nadam', 'adam', 'sgd'])}}, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=1024,\n",
    "              epochs=10,\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMBO in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from numpy import nan\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import wrangle as wr\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import optimizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import RandomizedSearchCV\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [8,9,10]),\n",
      "        'kernel_initializer': hp.choice('kernel_initializer', ['uniform', 'normal']),\n",
      "        'activation': hp.choice('activation', ['relu', 'elu']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'kernel_initializer_1': hp.choice('kernel_initializer_1', ['uniform', 'normal']),\n",
      "        'optimizer': hp.choice('optimizer', ['nadam', 'adam', 'sgd']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \"\"\"\n",
      "  3: Data providing function:\n",
      "  4: This function is separated from model() so that hyperopt\n",
      "  5: won't reload data for each evaluation run.\n",
      "  6: \"\"\"\n",
      "  7: # Base Url for the dataset\n",
      "  8: BASE = 'https://raw.githubusercontent.com/autonomio/datasets/master/autonomio-datasets/'\n",
      "  9: \n",
      " 10: def breast_cancer():\n",
      " 11:     '''Download and preprocess(cleaning) the dataset'''\n",
      " 12:     df = pd.read_csv(BASE + 'breast_cancer.csv')\n",
      " 13: \n",
      " 14:     # then some minimal data cleanup\n",
      " 15:     df.drop(\"Unnamed: 32\", axis=1, inplace=True)\n",
      " 16:     df.drop(\"id\", axis=1, inplace=True)\n",
      " 17: \n",
      " 18:     # separate to x and y\n",
      " 19:     y = df.diagnosis.values\n",
      " 20:     x = df.drop('diagnosis', axis=1).values\n",
      " 21: \n",
      " 22:     # convert the string labels to binary\n",
      " 23:     y = (y == 'M').astype(int)\n",
      " 24: \n",
      " 25:     return x, y\n",
      " 26: \n",
      " 27: # Load the dataset\n",
      " 28: x, y = breast_cancer()\n",
      " 29: \n",
      " 30: # Normalize every feature to mean 0, std 1\n",
      " 31: x = wr.mean_zero(pd.DataFrame(x)).values\n",
      " 32: \n",
      " 33: input_dim = x.shape[1] # number of columns\n",
      " 34: \n",
      " 35: x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=7)\n",
      " 36: \n",
      " 37: \n",
      " 38: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   6:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   7:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   8:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "   9:     The last one is optional, though recommended, namely:\n",
      "  10:         - model: specify the model just created so that we can later use it again.\n",
      "  11:     \"\"\"\n",
      "  12:     model = Sequential()\n",
      "  13:     \n",
      "  14:     # L1\n",
      "  15:     model.add(Dense(space['Dense'], \n",
      "  16:                     input_dim=input_dim, \n",
      "  17:                     kernel_initializer=space['kernel_initializer'], \n",
      "  18:                     activation=space['activation']))\n",
      "  19:     # Dropout\n",
      "  20:     model.add(Dropout(space['Dropout']))\n",
      "  21:     # L2\n",
      "  22:     model.add(Dense(1, \n",
      "  23:                     kernel_initializer=space['kernel_initializer_1'], \n",
      "  24:                     activation='sigmoid'))\n",
      "  25:     # Compile model\n",
      "  26:     model.compile(loss='binary_crossentropy', \n",
      "  27:                   optimizer=space['optimizer'], \n",
      "  28:                   metrics=['accuracy'])\n",
      "  29:     \n",
      "  30:     model.fit(x_train, y_train,\n",
      "  31:               batch_size=1024,\n",
      "  32:               epochs=10,\n",
      "  33:               verbose=2,\n",
      "  34:               validation_data=(x_test, y_test))\n",
      "  35:     \n",
      "  36:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  37:     print('Test accuracy:', acc)\n",
      "  38:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  39: \n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6936 - acc: 0.4829 - val_loss: 0.6946 - val_acc: 0.3404\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6938 - acc: 0.4829 - val_loss: 0.6943 - val_acc: 0.3404\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6940 - acc: 0.4777 - val_loss: 0.6940 - val_acc: 0.3404\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6934 - acc: 0.5328 - val_loss: 0.6937 - val_acc: 0.3830\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6930 - acc: 0.5276 - val_loss: 0.6934 - val_acc: 0.4202\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6921 - acc: 0.5643 - val_loss: 0.6931 - val_acc: 0.4574\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6924 - acc: 0.5512 - val_loss: 0.6928 - val_acc: 0.5213\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6923 - acc: 0.5381 - val_loss: 0.6925 - val_acc: 0.5638\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6922 - acc: 0.5459 - val_loss: 0.6923 - val_acc: 0.6064\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6919 - acc: 0.5144 - val_loss: 0.6920 - val_acc: 0.6436\n",
      "Test accuracy: 0.6436170200084118\n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6947 - acc: 0.5171 - val_loss: 0.6921 - val_acc: 0.6915\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6919 - acc: 0.5774 - val_loss: 0.6904 - val_acc: 0.7660\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6904 - acc: 0.5906 - val_loss: 0.6888 - val_acc: 0.7819\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6885 - acc: 0.5879 - val_loss: 0.6873 - val_acc: 0.8138\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6902 - acc: 0.6220 - val_loss: 0.6857 - val_acc: 0.8245\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6884 - acc: 0.6535 - val_loss: 0.6838 - val_acc: 0.8298\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6838 - acc: 0.6457 - val_loss: 0.6815 - val_acc: 0.8404\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6822 - acc: 0.6640 - val_loss: 0.6791 - val_acc: 0.8404\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6787 - acc: 0.6850 - val_loss: 0.6765 - val_acc: 0.8723\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6770 - acc: 0.6955 - val_loss: 0.6735 - val_acc: 0.8777\n",
      "Test accuracy: 0.8776595706635333\n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6942 - acc: 0.5932 - val_loss: 0.6916 - val_acc: 0.6436\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6962 - acc: 0.6037 - val_loss: 0.6905 - val_acc: 0.7926\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6937 - acc: 0.6037 - val_loss: 0.6894 - val_acc: 0.8670\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6869 - acc: 0.6378 - val_loss: 0.6882 - val_acc: 0.8989\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6931 - acc: 0.6063 - val_loss: 0.6871 - val_acc: 0.9096\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6893 - acc: 0.6299 - val_loss: 0.6860 - val_acc: 0.9202\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6831 - acc: 0.6509 - val_loss: 0.6849 - val_acc: 0.9255\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6814 - acc: 0.6509 - val_loss: 0.6837 - val_acc: 0.9309\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6865 - acc: 0.6273 - val_loss: 0.6825 - val_acc: 0.9362\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6868 - acc: 0.6299 - val_loss: 0.6814 - val_acc: 0.9362\n",
      "Test accuracy: 0.9361702127659575\n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.7066 - acc: 0.4908 - val_loss: 0.6991 - val_acc: 0.2234\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.7060 - acc: 0.4672 - val_loss: 0.6962 - val_acc: 0.3191\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6974 - acc: 0.4987 - val_loss: 0.6933 - val_acc: 0.4521\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6940 - acc: 0.5591 - val_loss: 0.6904 - val_acc: 0.5957\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6923 - acc: 0.5643 - val_loss: 0.6875 - val_acc: 0.7074\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6867 - acc: 0.6457 - val_loss: 0.6846 - val_acc: 0.7926\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6850 - acc: 0.6089 - val_loss: 0.6818 - val_acc: 0.8511\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6881 - acc: 0.6010 - val_loss: 0.6789 - val_acc: 0.8777\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6867 - acc: 0.5748 - val_loss: 0.6761 - val_acc: 0.8989\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6794 - acc: 0.6220 - val_loss: 0.6732 - val_acc: 0.8989\n",
      "Test accuracy: 0.898936170212766\n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6894 - acc: 0.6798 - val_loss: 0.6840 - val_acc: 0.8457\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6850 - acc: 0.7612 - val_loss: 0.6806 - val_acc: 0.8777\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6813 - acc: 0.8346 - val_loss: 0.6772 - val_acc: 0.9043\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6777 - acc: 0.8609 - val_loss: 0.6735 - val_acc: 0.9202\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6738 - acc: 0.8714 - val_loss: 0.6696 - val_acc: 0.9309\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6703 - acc: 0.8766 - val_loss: 0.6653 - val_acc: 0.9255\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6659 - acc: 0.8793 - val_loss: 0.6607 - val_acc: 0.9255\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6615 - acc: 0.8819 - val_loss: 0.6556 - val_acc: 0.9309\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6565 - acc: 0.8950 - val_loss: 0.6502 - val_acc: 0.9309\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6503 - acc: 0.8898 - val_loss: 0.6442 - val_acc: 0.9309\n",
      "Test accuracy: 0.9308510638297872\n"
     ]
    }
   ],
   "source": [
    "from hyperas import optim\n",
    "\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='bayesian_optimization_example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Let's see which configuration give us the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "188/188 [==============================] - 0s 39us/step\n",
      "[0.681391586648657, 0.9361702127659575]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dense': 2, 'Dropout': 0.9758185183456943, 'activation': 0, 'kernel_initializer': 0, 'kernel_initializer_1': 0, 'optimizer': 1}\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's all folks - don't forget to shutdown your workspace once you're done ðŸ™‚**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
