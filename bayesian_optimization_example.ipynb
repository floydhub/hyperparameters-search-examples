{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization Example\n",
    "\n",
    "In this example we will perform a Bayesian Optimization using [Hyperas](https://github.com/maxpumperla/hyperas) on the [breast cancer](https://github.com/autonomio/datasets/blob/master/autonomio-datasets/breast_cancer.csv) classification task. You can run this example on CPU. It will take more or less 3 minutes.\n",
    "\n",
    "We will continue to use the same example of the [Grid Search](./grid_search_example.ipynb) and [Random Search](./random_search_example.ipynb) notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "Import the packages we need for the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import wrangle as wr\n",
    "\n",
    "from numpy import nan\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Load, clean and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "    This function is separated from model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    # Mounting point\n",
    "    MP = '/floyd/input/bcds'\n",
    "    \n",
    "    def breast_cancer():\n",
    "        \"\"\"Download and preprocess(cleaning) the dataset\"\"\"\n",
    "        df = pd.read_csv(os.path.join(MP, 'breast_cancer.csv'))\n",
    "\n",
    "        # then some minimal data cleanup\n",
    "        df.drop(\"Unnamed: 32\", axis=1, inplace=True)\n",
    "        df.drop(\"id\", axis=1, inplace=True)\n",
    "\n",
    "        # separate to x and y\n",
    "        y = df.diagnosis.values\n",
    "        x = df.drop('diagnosis', axis=1).values\n",
    "\n",
    "        # convert the string labels to binary\n",
    "        y = (y == 'M').astype(int)\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    # Load the dataset\n",
    "    x, y = breast_cancer()\n",
    "\n",
    "    # Normalize every feature to mean 0, std 1\n",
    "    x = wr.mean_zero(pd.DataFrame(x)).values\n",
    "\n",
    "    input_dim = x.shape[1] # number of columns\n",
    "\n",
    "    # Train - Test split: 66 - 33\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=7)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "Define the model and the variables to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "def model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # L1\n",
    "    model.add(Dense({{choice([8,9,10])}}, \n",
    "                    input_dim=input_dim, \n",
    "                    kernel_initializer={{choice(['uniform', 'normal'])}}, \n",
    "                    activation={{choice(['relu', 'elu'])}}))\n",
    "    # Dropout\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    # L2\n",
    "    model.add(Dense(1, \n",
    "                    kernel_initializer={{choice(['uniform', 'normal'])}}, \n",
    "                    activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer={{choice(['nadam', 'adam', 'sgd'])}}, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=1024,\n",
    "              epochs=10,\n",
    "              verbose=2,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMBO in action\n",
    "\n",
    "Run 5 iterations using using the [Tree Parzen Estimator or TPE algorithm](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) provided with [hyperopt](https://github.com/hyperopt/hyperopt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import wrangle as wr\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from numpy import nan\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [8,9,10]),\n",
      "        'kernel_initializer': hp.choice('kernel_initializer', ['uniform', 'normal']),\n",
      "        'activation': hp.choice('activation', ['relu', 'elu']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'kernel_initializer_1': hp.choice('kernel_initializer_1', ['uniform', 'normal']),\n",
      "        'optimizer': hp.choice('optimizer', ['nadam', 'adam', 'sgd']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: \"\"\"\n",
      "  3: Data providing function:\n",
      "  4: This function is separated from model() so that hyperopt\n",
      "  5: won't reload data for each evaluation run.\n",
      "  6: \"\"\"\n",
      "  7: # Mounting point\n",
      "  8: MP = '/floyd/input/bcds'\n",
      "  9: \n",
      " 10: def breast_cancer():\n",
      " 11:     \"\"\"Download and preprocess(cleaning) the dataset\"\"\"\n",
      " 12:     df = pd.read_csv(os.path.join(MP, 'breast_cancer.csv'))\n",
      " 13: \n",
      " 14:     # then some minimal data cleanup\n",
      " 15:     df.drop(\"Unnamed: 32\", axis=1, inplace=True)\n",
      " 16:     df.drop(\"id\", axis=1, inplace=True)\n",
      " 17: \n",
      " 18:     # separate to x and y\n",
      " 19:     y = df.diagnosis.values\n",
      " 20:     x = df.drop('diagnosis', axis=1).values\n",
      " 21: \n",
      " 22:     # convert the string labels to binary\n",
      " 23:     y = (y == 'M').astype(int)\n",
      " 24: \n",
      " 25:     return x, y\n",
      " 26: \n",
      " 27: # Load the dataset\n",
      " 28: x, y = breast_cancer()\n",
      " 29: \n",
      " 30: # Normalize every feature to mean 0, std 1\n",
      " 31: x = wr.mean_zero(pd.DataFrame(x)).values\n",
      " 32: \n",
      " 33: input_dim = x.shape[1] # number of columns\n",
      " 34: \n",
      " 35: # Train - Test split: 66 - 33\n",
      " 36: x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=7)\n",
      " 37: \n",
      " 38: \n",
      " 39: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   6:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   7:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   8:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "   9:     The last one is optional, though recommended, namely:\n",
      "  10:         - model: specify the model just created so that we can later use it again.\n",
      "  11:     \"\"\"\n",
      "  12:     model = Sequential()\n",
      "  13:     \n",
      "  14:     # L1\n",
      "  15:     model.add(Dense(space['Dense'], \n",
      "  16:                     input_dim=input_dim, \n",
      "  17:                     kernel_initializer=space['kernel_initializer'], \n",
      "  18:                     activation=space['activation']))\n",
      "  19:     # Dropout\n",
      "  20:     model.add(Dropout(space['Dropout']))\n",
      "  21:     # L2\n",
      "  22:     model.add(Dense(1, \n",
      "  23:                     kernel_initializer=space['kernel_initializer_1'], \n",
      "  24:                     activation='sigmoid'))\n",
      "  25:     # Compile model\n",
      "  26:     model.compile(loss='binary_crossentropy', \n",
      "  27:                   optimizer=space['optimizer'], \n",
      "  28:                   metrics=['accuracy'])\n",
      "  29:     \n",
      "  30:     model.fit(x_train, y_train,\n",
      "  31:               batch_size=1024,\n",
      "  32:               epochs=10,\n",
      "  33:               verbose=2,\n",
      "  34:               validation_data=(x_test, y_test))\n",
      "  35:     \n",
      "  36:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  37:     print('Test accuracy:', acc)\n",
      "  38:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  39: \n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.6865 - acc: 0.5906 - val_loss: 0.6899 - val_acc: 0.6383\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6883 - acc: 0.5722 - val_loss: 0.6895 - val_acc: 0.6755\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6871 - acc: 0.6352 - val_loss: 0.6890 - val_acc: 0.6968\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6884 - acc: 0.6142 - val_loss: 0.6885 - val_acc: 0.7021\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6875 - acc: 0.6115 - val_loss: 0.6880 - val_acc: 0.7553\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6873 - acc: 0.6378 - val_loss: 0.6876 - val_acc: 0.8085\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6845 - acc: 0.6667 - val_loss: 0.6871 - val_acc: 0.8191\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6847 - acc: 0.6352 - val_loss: 0.6866 - val_acc: 0.8511\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6835 - acc: 0.7060 - val_loss: 0.6862 - val_acc: 0.8723\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6845 - acc: 0.6772 - val_loss: 0.6857 - val_acc: 0.8723\n",
      "Test accuracy: 0.8723404217273631\n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.6989 - acc: 0.3858 - val_loss: 0.6985 - val_acc: 0.2766\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6980 - acc: 0.4541 - val_loss: 0.6964 - val_acc: 0.3138\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6948 - acc: 0.4777 - val_loss: 0.6944 - val_acc: 0.3883\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6913 - acc: 0.5354 - val_loss: 0.6924 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6898 - acc: 0.5302 - val_loss: 0.6904 - val_acc: 0.6383\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6903 - acc: 0.5092 - val_loss: 0.6883 - val_acc: 0.7340\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6871 - acc: 0.5748 - val_loss: 0.6861 - val_acc: 0.8723\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6827 - acc: 0.6115 - val_loss: 0.6836 - val_acc: 0.9096\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6841 - acc: 0.6142 - val_loss: 0.6813 - val_acc: 0.9202\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6783 - acc: 0.6693 - val_loss: 0.6786 - val_acc: 0.9309\n",
      "Test accuracy: 0.9308510663661551\n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.6939 - acc: 0.5591 - val_loss: 0.6927 - val_acc: 0.6223\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6911 - acc: 0.5722 - val_loss: 0.6919 - val_acc: 0.7021\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6900 - acc: 0.5984 - val_loss: 0.6911 - val_acc: 0.7713\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6915 - acc: 0.5958 - val_loss: 0.6904 - val_acc: 0.7872\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6902 - acc: 0.5906 - val_loss: 0.6896 - val_acc: 0.8085\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6899 - acc: 0.6168 - val_loss: 0.6888 - val_acc: 0.8351\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6883 - acc: 0.6115 - val_loss: 0.6880 - val_acc: 0.8404\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6895 - acc: 0.6089 - val_loss: 0.6872 - val_acc: 0.8564\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6862 - acc: 0.6194 - val_loss: 0.6864 - val_acc: 0.8564\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6856 - acc: 0.6247 - val_loss: 0.6856 - val_acc: 0.8617\n",
      "Test accuracy: 0.8617021301959423\n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.6838 - acc: 0.6430 - val_loss: 0.6787 - val_acc: 0.7819\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6852 - acc: 0.6352 - val_loss: 0.6759 - val_acc: 0.8245\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6782 - acc: 0.6850 - val_loss: 0.6729 - val_acc: 0.8404\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6749 - acc: 0.6877 - val_loss: 0.6700 - val_acc: 0.8617\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6668 - acc: 0.7323 - val_loss: 0.6670 - val_acc: 0.8670\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6691 - acc: 0.7244 - val_loss: 0.6639 - val_acc: 0.8883\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6681 - acc: 0.7297 - val_loss: 0.6608 - val_acc: 0.8936\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6640 - acc: 0.7822 - val_loss: 0.6577 - val_acc: 0.8989\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6616 - acc: 0.7349 - val_loss: 0.6545 - val_acc: 0.9043\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6633 - acc: 0.7533 - val_loss: 0.6513 - val_acc: 0.9043\n",
      "Test accuracy: 0.904255321685304\n",
      "Train on 381 samples, validate on 188 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.6913 - acc: 0.6404 - val_loss: 0.6856 - val_acc: 0.7766\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6858 - acc: 0.7769 - val_loss: 0.6813 - val_acc: 0.8404\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6817 - acc: 0.8268 - val_loss: 0.6771 - val_acc: 0.8723\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6774 - acc: 0.8793 - val_loss: 0.6728 - val_acc: 0.8936\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6731 - acc: 0.8819 - val_loss: 0.6682 - val_acc: 0.8936\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6683 - acc: 0.8898 - val_loss: 0.6632 - val_acc: 0.8989\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6635 - acc: 0.8950 - val_loss: 0.6579 - val_acc: 0.9043\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6583 - acc: 0.8976 - val_loss: 0.6522 - val_acc: 0.9043\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6524 - acc: 0.9108 - val_loss: 0.6462 - val_acc: 0.9096\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6463 - acc: 0.9055 - val_loss: 0.6399 - val_acc: 0.9096\n",
      "Test accuracy: 0.9095744706214742\n"
     ]
    }
   ],
   "source": [
    "from hyperas import optim\n",
    "\n",
    "best_run, best_model = optim.minimize(model=model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=5,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='bayesian_optimization_example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Let's see which configuration give us the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of best performing model:\n",
      "188/188 [==============================] - 0s 32us/step\n",
      "[0.6786171705164807, 0.9308510663661551]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dense': 2, 'Dropout': 0.8366666847115819, 'activation': 0, 'kernel_initializer': 1, 'kernel_initializer_1': 0, 'optimizer': 0}\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(x_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That's all folks - don't forget to shutdown your workspace once you're done 🙂**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
